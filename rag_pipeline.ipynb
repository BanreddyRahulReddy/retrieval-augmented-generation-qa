{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0d13d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2d01a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory created at: /content/drive/MyDrive/rag_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/rag_project\"\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, \"data\")\n",
    "\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Project directory created at:\", PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d1851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data folder:\n",
      "['cs231n_full_notes.pdf', 'cnn_transformers_intro.pdf', 'cs224n_transformers_2024.pdf', 'attention_is_all_you_need.pdf', 'cs224n_merged_notes.pdf', 'neural_networks_backprop.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Files in data folder:\")\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97925d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.27.1-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu128)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.1)\n",
      "Requirement already satisfied: typer>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.23.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (0.1.2)\n",
      "Downloading pymupdf-1.27.1-cp310-abi3-manylinux_2_28_x86_64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=674f34f5859a5fe0512cfcca8cc42b02eaca61717f66872c10e9f189a3026165\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: pymupdf, faiss-cpu, rouge-score\n",
      "Successfully installed faiss-cpu-1.13.2 pymupdf-1.27.1 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf sentence-transformers faiss-cpu transformers rouge-score scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2c17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8a0aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: cs231n_full_notes.pdf | Characters: 149765\n",
      "Extracted: cnn_transformers_intro.pdf | Characters: 55110\n",
      "Extracted: cs224n_transformers_2024.pdf | Characters: 27641\n",
      "Extracted: attention_is_all_you_need.pdf | Characters: 32708\n",
      "Extracted: cs224n_merged_notes.pdf | Characters: 234082\n",
      "Extracted: neural_networks_backprop.pdf | Characters: 30428\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Collapse excessive whitespace and strip leading/trailing spaces.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_pdfs_from_folder(data_path):\n",
    "    \"\"\"Extract and clean raw text from every PDF in *data_path*.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping filename -> cleaned full text.\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(data_path, file)\n",
    "            doc = fitz.open(file_path)\n",
    "            full_text = \"\"\n",
    "            for page in doc:\n",
    "                full_text += page.get_text()\n",
    "            full_text = clean_text(full_text)\n",
    "            documents[file] = full_text\n",
    "            print(f\"Extracted: {file} | Characters: {len(full_text)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = extract_pdfs_from_folder(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba066383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview from: cs231n_full_notes.pdf\n",
      "DEEP LEARNING STUDY NOTES All credits go to L. Fei-Fei, A. Karpathy, J.Johnson teachers of the CS231n course. Thank you for this amazing course!! by Albert Pumarola Contents I DATA 7 1 Data Preprocessing 9 2 Making the most of your data - Data Augmentation and Transfer Learning 11 2.1 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2 Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 II LEARNING 15 3 Neural Network 17 4 Parameters Initialization 21 4.1 Weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2 Biases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5 Activation Function 25 5.1 Sigmoid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2 Tanh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.3 ReLU . . . . . . . . . . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "# Preview one document\n",
    "sample_file = list(documents.keys())[0]\n",
    "print(\"Preview from:\", sample_file)\n",
    "print(documents[sample_file][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abff31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1128\n",
      "Avg chunk length (words): 113\n",
      "Avg chunk length (chars): 640\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=120, overlap=30):\n",
    "    \"\"\"Sentence-boundary-aware chunking with word-level size control.\n",
    "\n",
    "    Splits text into chunks of approximately *chunk_size* words, never\n",
    "    breaking mid-sentence.  An overlap of *overlap* words is carried\n",
    "    forward between consecutive chunks so that cross-boundary context\n",
    "    is preserved.\n",
    "\n",
    "    The default 120-word / 30-word-overlap setting is chosen to align\n",
    "    with the token capacity of all-MiniLM-L6-v2, which produces its\n",
    "    best embeddings at roughly 100-128 tokens (~100-120 words).\n",
    "\n",
    "    Args:\n",
    "        text: Raw document string.\n",
    "        chunk_size: Target number of words per chunk (default 120).\n",
    "        overlap: Number of trailing words to carry into the next chunk.\n",
    "\n",
    "    Returns:\n",
    "        List of chunk strings.\n",
    "    \"\"\"\n",
    "    # Split into sentences first for boundary-aware chunking\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_words = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split()\n",
    "        sentence_word_count = len(sentence_words)\n",
    "\n",
    "        # If adding this sentence exceeds chunk_size, finalize current chunk\n",
    "        if current_word_count + sentence_word_count > chunk_size and current_chunk_words:\n",
    "            chunks.append(\" \".join(current_chunk_words))\n",
    "\n",
    "            # Overlap: keep last `overlap` words for continuity\n",
    "            overlap_words = (\n",
    "                current_chunk_words[-overlap:]\n",
    "                if overlap < len(current_chunk_words)\n",
    "                else current_chunk_words\n",
    "            )\n",
    "            current_chunk_words = list(overlap_words)\n",
    "            current_word_count = len(current_chunk_words)\n",
    "\n",
    "        current_chunk_words.extend(sentence_words)\n",
    "        current_word_count += sentence_word_count\n",
    "\n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk_words:\n",
    "        chunks.append(\" \".join(current_chunk_words))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ── Apply chunking to all documents ──────────────────────────\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for filename, text in documents.items():\n",
    "    chunks = chunk_text(text, chunk_size=120, overlap=30)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_metadata.append({\"source\": filename, \"chunk_index\": i})\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Avg chunk length (words): {np.mean([len(c.split()) for c in all_chunks]):.0f}\")\n",
    "print(f\"Avg chunk length (chars): {np.mean([len(c) for c in all_chunks]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91cce1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      "DEEP LEARNING STUDY NOTES All credits go to L. Fei-Fei, A. Karpathy, J.Johnson teachers of the CS231n course. Thank you for this amazing course!! by Albert Pumarola Contents I DATA 7 1 Data Preprocessing 9 2 Making the most of your data - Data Augmentation and Transfer Learning 11 2.1 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2 Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample chunk:\")\n",
    "print(all_chunks[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d9e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fc5f20c4af44259fd4ea590de3f6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a631d77a60544762bf32e148ad649a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38121933312a47a490bc45c617f5e16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887e3508b3674ce0b2d2b91c32a04460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c000e0ed254a88b0773c17e7189c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f7dff5378349cc91343d8334593c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832d5d3437784b87ac81e9f7e53e95f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b65fc5d57c64c60800a046830f3fb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022fd0523ac84715ab06127e24d90735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546b8de9d12d4f138d2548ed7e458c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cf29f0f57f42bf9b12b58472662a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff12ea57d3cb49adb052ed56d960a241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09339eb7f2b04c06bb3ce88f6701fceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1128, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Generate embeddings\n",
    "chunk_embeddings = embedding_model.encode(\n",
    "    all_chunks,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398d0599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors in index: 1128\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Dimension of embeddings\n",
    "dimension = chunk_embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b90dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 | Distance: 0.5534 | Source: {'source': 'cs231n_full_notes.pdf', 'chunk_index': 130} ---\n",
      "y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to diﬀerent parts of the circuit, then the gradients that ﬂow back to it will add. Patterns in backward ﬂow It is interesting to note that in many cases the backward-ﬂowing gradient can be interpreted on an intuitive level. \n",
      "\n",
      "--- Chunk 2 | Distance: 0.6310 | Source: {'source': 'cs231n_full_notes.pdf', 'chunk_index': 107} ---\n",
      "Notice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in. However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradie\n",
      "\n",
      "--- Chunk 3 | Distance: 0.7360 | Source: {'source': 'cs224n_merged_notes.pdf', 'chunk_index': 148} ---\n",
      "−α∇θ(t) J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed-forward computation on the model. To understand this further, let us understand the toy network shown in Figure 5 for which we will perform backpropagation. Figure 5: This is a 4-2-1 neural network where neuron j on layer k receives input z(k) j and produces activation output a(k) j . cs224n: natural language processing with deep learning 5\n",
      "\n",
      "--- Chunk 4 | Distance: 0.7366 | Source: {'source': 'cs231n_full_notes.pdf', 'chunk_index': 91} ---\n",
      "least skim this section, since it presents a rarely developed view of backpropagation as backward ﬂow in real-valued circuits and any insights youll gain may help you throughout the class. 33 Simple expressions and interpretation of the gradient Lets start simple so that we can develop the notation and conventions for more complex expres- sions. Consider a simple multiplication function of two numbers f(x, y) = xy.\n",
      "\n",
      "--- Chunk 5 | Distance: 0.7637 | Source: {'source': 'cs231n_full_notes.pdf', 'chunk_index': 121} ---\n",
      "between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables. The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the lea\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, top_k=5, score_threshold=None):\n",
    "    \"\"\"Retrieve the top-k nearest chunks for *query* from the FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query: Natural-language question string.\n",
    "        top_k: Number of nearest neighbours to return.\n",
    "        score_threshold: Optional L2 distance ceiling; chunks above this\n",
    "            threshold are discarded as irrelevant.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts, each containing 'chunk', 'distance', and 'metadata'.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx == -1:  # FAISS returns -1 for missing results\n",
    "            continue\n",
    "        if score_threshold is not None and dist > score_threshold:\n",
    "            continue\n",
    "        results.append({\n",
    "            \"chunk\": all_chunks[idx],\n",
    "            \"distance\": float(dist),\n",
    "            \"metadata\": chunk_metadata[idx],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# ── Test retrieval ────────────────────────────────────────────\n",
    "test_query = \"What is backpropagation?\"\n",
    "retrieved = retrieve(test_query, top_k=5)\n",
    "\n",
    "for i, r in enumerate(retrieved):\n",
    "    print(f\"\\n--- Chunk {i+1} | Distance: {r['distance']:.4f} | Source: {r['metadata']} ---\")\n",
    "    print(r['chunk'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5fe209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8e887df1ad4bbfa8a04a31b81584ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ededd6b97f424c8e8eae9bce275d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e42b8bf4470440887fc7fef770eb22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3c8aae65b54e8888f5c34c142b9f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f071e5e637c24037b3d90f801f841f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aa171c2b69426aab9055bb9a0a9cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87356b6509aa41d2bb05b970cda9968b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c75f07463254a14bfb1c2fff38f4abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory used: 2.00 GB\n",
      "Generator model loaded: google/flan-t5-large\n",
      "Parameters: 783M\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# A) UPGRADED MODEL LOADING — flan-t5-large (780M params)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# WHY: flan-t5-base (250M) produces extractive fragments instead of\n",
    "#       synthesized paragraph answers. flan-t5-large (780M) has 3x\n",
    "#       more parameters, significantly better instruction-following,\n",
    "#       and fits comfortably on a Colab T4 GPU (~1.5 GB in float16).\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16  # Half-precision to fit Colab GPU memory\n",
    ")\n",
    "\n",
    "generator_model = generator_model.to(device)\n",
    "generator_model.eval()\n",
    "\n",
    "# Memory check\n",
    "if device == 'cuda':\n",
    "    mem_gb = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"GPU memory used: {mem_gb:.2f} GB\")\n",
    "\n",
    "print(f\"Generator model loaded: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in generator_model.parameters()) / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95e6ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation config (shared): {'max_new_tokens': 220, 'min_new_tokens': 60, 'num_beams': 4, 'length_penalty': 1.0, 'no_repeat_ngram_size': 3, 'do_sample': False}\n",
      "Prompt template (shared): identical for RAG and baseline\n",
      "Functions defined: generate_rag_answer(), generate_baseline_answer()\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# B) RAG GENERATION + BASELINE (FAIR COMPARISON)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Both functions share identical decoding parameters AND the same\n",
    "# prompt template via GENERATION_CONFIG and PROMPT_TEMPLATE.\n",
    "# The only difference: RAG fills context with retrieved chunks;\n",
    "# baseline passes an empty string.  This eliminates the prompt-\n",
    "# wording confound that could inflate or deflate either system.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 220,\n",
    "    \"min_new_tokens\": 60,\n",
    "    \"num_beams\": 4,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Use the information below to answer the question clearly and factually.\\n\"\n",
    "    \"If the information is insufficient, say that the answer cannot be determined \"\n",
    "    \"from the provided material.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question:\\n{question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_rag_answer(query, top_k=3, max_context_tokens=300):\n",
    "    \"\"\"Generate an answer using retrieved context (RAG pipeline).\n",
    "\n",
    "    The prompt is built from PROMPT_TEMPLATE with retrieved chunks as\n",
    "    context.  Decoding uses GENERATION_CONFIG (shared with baseline).\n",
    "\n",
    "    Args:\n",
    "        query: User question string.\n",
    "        top_k: Number of chunks to retrieve.\n",
    "        max_context_tokens: Max tokens allocated to context in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        Dict with answer, sources, num_chunks_used, and input_tokens.\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join(\n",
    "        rc[\"chunk\"] if isinstance(rc, dict) else rc for rc in retrieved_chunks\n",
    "    )\n",
    "\n",
    "    context_tokens = tokenizer(\n",
    "        context, return_tensors=\"pt\", truncation=True, max_length=max_context_tokens\n",
    "    )\n",
    "    truncated_context = tokenizer.decode(\n",
    "        context_tokens[\"input_ids\"][0], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(context=truncated_context, question=query)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(**inputs, **GENERATION_CONFIG)\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    sources = [rc[\"metadata\"] for rc in retrieved_chunks if isinstance(rc, dict)]\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"num_chunks_used\": len(retrieved_chunks),\n",
    "        \"input_tokens\": inputs[\"input_ids\"].shape[1],\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_baseline_answer(query):\n",
    "    \"\"\"Generate an answer WITHOUT retrieval context (parametric baseline).\n",
    "\n",
    "    Uses the SAME PROMPT_TEMPLATE as RAG but with an empty context\n",
    "    string, and the same GENERATION_CONFIG, ensuring a fair comparison.\n",
    "\n",
    "    Args:\n",
    "        query: User question string.\n",
    "\n",
    "    Returns:\n",
    "        Dict with answer.\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_TEMPLATE.format(context=\"\", question=query)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(**inputs, **GENERATION_CONFIG)\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "print(f\"Generation config (shared): {GENERATION_CONFIG}\")\n",
    "print(f\"Prompt template (shared): identical for RAG and baseline\")\n",
    "print(\"Functions defined: generate_rag_answer(), generate_baseline_answer()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb89f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is backpropagation?\n",
      "Input tokens used: 339/512\n",
      "Chunks used: 3\n",
      "Sources: [{'source': 'cs231n_full_notes.pdf', 'chunk_index': 130}, {'source': 'cs231n_full_notes.pdf', 'chunk_index': 107}, {'source': 'cs224n_merged_notes.pdf', 'chunk_index': 148}]\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "RAG Answer:\n",
      "technique that allows us to use the chain rule of differentiation to learn about the local gradient of its inputs with respect to its output value. (t) J Backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its input value and 2. the local\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Baseline Answer (no retrieval):\n",
      "can not be determined from the provided material. Backpropagation refers to an algorithm used to propagate directional information from one point to the next point in a network. Back propagation is based on directional propagation theory. Back propagation is an algorithm that can propagate information from a point to another point in the network.\n"
     ]
    }
   ],
   "source": [
    "# ── Quick sanity check: RAG vs Baseline on one question ──\n",
    "question = \"What is backpropagation?\"\n",
    "\n",
    "rag_result = generate_rag_answer(question)\n",
    "baseline_result = generate_baseline_answer(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Input tokens used: {rag_result['input_tokens']}/512\")\n",
    "print(f\"Chunks used: {rag_result['num_chunks_used']}\")\n",
    "print(f\"Sources: {rag_result['sources']}\")\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\"RAG Answer:\\n{rag_result['answer']}\")\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\"Baseline Answer (no retrieval):\\n{baseline_result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ede8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c640187e534646bdcbae760d2c36cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9306bb64d4654e4e81e6b26a966bdee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0510e2a90e84942bb63f4695d1c201c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad364afc1fc948a5bb0320d54be9a63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecf9fe3de994df589ef58dcec22d920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d03368b2aa41729a32cf275d1dbfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a45bf7a09264978b2d3a17ec8e71597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaModel LOAD REPORT from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset integrity checks passed.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# E1) REPRODUCIBILITY + METRICS + 30-QUESTION EVALUATION DATASET\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel, t\n",
    "\n",
    "# Install bert-score once, then preload BERTScorer\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"bert-score\", \"-q\"], check=True)\n",
    "\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "device_for_bert = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bert_scorer = BERTScorer(\n",
    "    model_type=\"roberta-large\",\n",
    "    lang=\"en\",\n",
    "    rescale_with_baseline=True,\n",
    "    device=device_for_bert\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_rouge_l(prediction, reference):\n",
    "    return scorer.score(reference, prediction)[\"rougeL\"].fmeasure\n",
    "\n",
    "def compute_cosine_similarity(text_a, text_b):\n",
    "    emb = embedding_model.encode([text_a, text_b], convert_to_numpy=True)\n",
    "    return float(cosine_similarity([emb[0]], [emb[1]])[0][0])\n",
    "\n",
    "def compute_bert_score(prediction, reference):\n",
    "    P, R, F1 = bert_scorer.score([prediction], [reference])\n",
    "    return float(F1[0])\n",
    "\n",
    "def evaluate_answer(prediction, reference):\n",
    "    return {\n",
    "        \"rouge_l\": compute_rouge_l(prediction, reference),\n",
    "        \"cosine_similarity\": compute_cosine_similarity(prediction, reference),\n",
    "        \"bert_score\": compute_bert_score(prediction, reference),\n",
    "    }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Dataset Definition (30 total: 20 standard, 5 multi-hop, 5 unanswerable)\n",
    "# Split: 20 dev, 10 test\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "KNOWN_SOURCES = {\n",
    "    \"cnn_transformers_intro.pdf\",\n",
    "    \"neural_networks_backprop.pdf\",\n",
    "    \"cs231n_full_notes.pdf\",\n",
    "    \"attention_is_all_you_need.pdf\",\n",
    "    \"cs224n_transformers_2024.pdf\",\n",
    "    \"cs224n_merged_notes.pdf\",\n",
    "}\n",
    "\n",
    "evaluation_dataset = [\n",
    "\n",
    "# STANDARD (20)\n",
    "{\"id\":\"S01\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is a perceptron?\",\n",
    " \"reference_answer\":\"A perceptron is a single linear neuron computing a weighted sum followed by thresholding.\",\n",
    " \"relevant_sources\":[\"neural_networks_backprop.pdf\"]},\n",
    "\n",
    "{\"id\":\"S02\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"Why are non-linear activations necessary?\",\n",
    " \"reference_answer\":\"Without non-linear activations, stacked linear layers collapse into one linear function.\",\n",
    " \"relevant_sources\":[\"neural_networks_backprop.pdf\"]},\n",
    "\n",
    "{\"id\":\"S03\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is the vanishing gradient problem?\",\n",
    " \"reference_answer\":\"Gradients shrink across many layers, slowing early-layer learning.\",\n",
    " \"relevant_sources\":[\"neural_networks_backprop.pdf\"]},\n",
    "\n",
    "{\"id\":\"S04\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What does learning rate control?\",\n",
    " \"reference_answer\":\"It controls update step size during optimization.\",\n",
    " \"relevant_sources\":[\"neural_networks_backprop.pdf\"]},\n",
    "\n",
    "{\"id\":\"S05\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is overfitting?\",\n",
    " \"reference_answer\":\"Overfitting occurs when a model memorizes training data.\",\n",
    " \"relevant_sources\":[\"cs231n_full_notes.pdf\"]},\n",
    "\n",
    "{\"id\":\"S06\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is layer normalization?\",\n",
    " \"reference_answer\":\"Layer normalization normalizes features within each example.\",\n",
    " \"relevant_sources\":[\"cs224n_transformers_2024.pdf\"]},\n",
    "\n",
    "{\"id\":\"S07\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"Why are CNNs parameter efficient?\",\n",
    " \"reference_answer\":\"Local connectivity and weight sharing reduce parameters.\",\n",
    " \"relevant_sources\":[\"cs231n_full_notes.pdf\"]},\n",
    "\n",
    "{\"id\":\"S08\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is pooling?\",\n",
    " \"reference_answer\":\"Pooling downsamples feature maps to reduce computation.\",\n",
    " \"relevant_sources\":[\"cs231n_full_notes.pdf\"]},\n",
    "\n",
    "{\"id\":\"S09\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is residual connection?\",\n",
    " \"reference_answer\":\"Residual connections allow gradients to pass directly across layers.\",\n",
    " \"relevant_sources\":[\"cs224n_transformers_2024.pdf\"]},\n",
    "\n",
    "{\"id\":\"S10\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"How does self-attention work?\",\n",
    " \"reference_answer\":\"Self-attention computes weighted sums of value vectors using query-key similarity.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"S11\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"Why multi-head attention?\",\n",
    " \"reference_answer\":\"Multiple heads capture diverse relational patterns.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"S12\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"Why positional encodings?\",\n",
    " \"reference_answer\":\"They inject order information into attention-based models.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"S13\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"What is scaled dot-product attention?\",\n",
    " \"reference_answer\":\"It scales query-key dot products before softmax normalization.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"S14\",\"split\":\"dev\",\"type\":\"standard\",\n",
    " \"question\":\"Why masking in decoder?\",\n",
    " \"reference_answer\":\"Masking prevents attending to future tokens.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "# TEST standard (6)\n",
    "{\"id\":\"S15\",\"split\":\"test\",\"type\":\"standard\",\n",
    " \"question\":\"What is teacher forcing?\",\n",
    " \"reference_answer\":\"Teacher forcing feeds ground-truth tokens during training.\",\n",
    " \"relevant_sources\":[\"cs224n_merged_notes.pdf\"]},\n",
    "\n",
    "{\"id\":\"S16\",\"split\":\"test\",\"type\":\"standard\",\n",
    " \"question\":\"Why use Adam optimizer?\",\n",
    " \"reference_answer\":\"Adam adapts learning rates per parameter.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"S17\",\"split\":\"test\",\"type\":\"standard\",\n",
    " \"question\":\"Why transfer learning helps?\",\n",
    " \"reference_answer\":\"Pretrained models improve performance with limited labeled data.\",\n",
    " \"relevant_sources\":[\"cs224n_transformers_2024.pdf\"]},\n",
    "\n",
    "{\"id\":\"S18\",\"split\":\"test\",\"type\":\"standard\",\n",
    " \"question\":\"What is encoder-decoder difference?\",\n",
    " \"reference_answer\":\"Encoders process input; decoders generate output autoregressively.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"S19\",\"split\":\"test\",\"type\":\"standard\",\n",
    " \"question\":\"What is stride in CNN?\",\n",
    " \"reference_answer\":\"Stride controls filter movement and spatial downsampling.\",\n",
    " \"relevant_sources\":[\"cs231n_full_notes.pdf\"]},\n",
    "\n",
    "{\"id\":\"S20\",\"split\":\"test\",\"type\":\"standard\",\n",
    " \"question\":\"Why 1x1 convolution?\",\n",
    " \"reference_answer\":\"It mixes channel information efficiently.\",\n",
    " \"relevant_sources\":[\"cs231n_full_notes.pdf\"]},\n",
    "\n",
    "# MULTI-HOP (5)\n",
    "{\"id\":\"M01\",\"split\":\"dev\",\"type\":\"multi_hop\",\n",
    " \"question\":\"How do residuals and normalization stabilize transformers?\",\n",
    " \"reference_answer\":\"Residual paths aid gradient flow while normalization stabilizes activations.\",\n",
    " \"relevant_sources\":[\"cs224n_transformers_2024.pdf\"]},\n",
    "\n",
    "{\"id\":\"M02\",\"split\":\"dev\",\"type\":\"multi_hop\",\n",
    " \"question\":\"Compare CNN locality and transformer attention.\",\n",
    " \"reference_answer\":\"CNNs model local patterns; transformers model global dependencies.\",\n",
    " \"relevant_sources\":[\"cs231n_full_notes.pdf\",\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"M03\",\"split\":\"dev\",\"type\":\"multi_hop\",\n",
    " \"question\":\"How does backprop update attention?\",\n",
    " \"reference_answer\":\"Gradients flow backward through attention projections and softmax.\",\n",
    " \"relevant_sources\":[\"neural_networks_backprop.pdf\",\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"M04\",\"split\":\"test\",\"type\":\"multi_hop\",\n",
    " \"question\":\"Why positional encodings despite permutation invariance?\",\n",
    " \"reference_answer\":\"They break symmetry so order-sensitive meaning can be learned.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "{\"id\":\"M05\",\"split\":\"test\",\"type\":\"multi_hop\",\n",
    " \"question\":\"How do Adam and LR schedules interact?\",\n",
    " \"reference_answer\":\"Adam adapts step sizes while schedules control overall training stability.\",\n",
    " \"relevant_sources\":[\"attention_is_all_you_need.pdf\"]},\n",
    "\n",
    "# UNANSWERABLE (5)\n",
    "{\"id\":\"U01\",\"split\":\"dev\",\"type\":\"unanswerable\",\n",
    " \"question\":\"What is CRISPR?\",\n",
    " \"reference_answer\":\"This cannot be answered from the provided corpus.\",\n",
    " \"relevant_sources\":[]},\n",
    "\n",
    "{\"id\":\"U02\",\"split\":\"dev\",\"type\":\"unanswerable\",\n",
    " \"question\":\"What is the capital of Brazil?\",\n",
    " \"reference_answer\":\"This cannot be answered from the provided corpus.\",\n",
    " \"relevant_sources\":[]},\n",
    "\n",
    "{\"id\":\"U03\",\"split\":\"dev\",\"type\":\"unanswerable\",\n",
    " \"question\":\"What caused the 2008 crisis?\",\n",
    " \"reference_answer\":\"This cannot be answered from the provided corpus.\",\n",
    " \"relevant_sources\":[]},\n",
    "\n",
    "{\"id\":\"U04\",\"split\":\"test\",\"type\":\"unanswerable\",\n",
    " \"question\":\"Who won the 2026 Super Bowl?\",\n",
    " \"reference_answer\":\"This cannot be answered from the provided corpus.\",\n",
    " \"relevant_sources\":[]},\n",
    "\n",
    "{\"id\":\"U05\",\"split\":\"test\",\"type\":\"unanswerable\",\n",
    " \"question\":\"Explain quantum error correction.\",\n",
    " \"reference_answer\":\"This cannot be answered from the provided corpus.\",\n",
    " \"relevant_sources\":[]},\n",
    "]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Dataset Validation\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "dev_set = [x for x in evaluation_dataset if x[\"split\"] == \"dev\"]\n",
    "test_set = [x for x in evaluation_dataset if x[\"split\"] == \"test\"]\n",
    "\n",
    "assert len(evaluation_dataset) == 30\n",
    "assert len(dev_set) == 20\n",
    "assert len(test_set) == 10\n",
    "\n",
    "type_counts = {\n",
    "    t: sum(1 for x in evaluation_dataset if x[\"type\"] == t)\n",
    "    for t in [\"standard\", \"multi_hop\", \"unanswerable\"]\n",
    "}\n",
    "\n",
    "assert type_counts[\"standard\"] == 20\n",
    "assert type_counts[\"multi_hop\"] == 5\n",
    "assert type_counts[\"unanswerable\"] == 5\n",
    "\n",
    "for item in evaluation_dataset:\n",
    "    if item[\"type\"] == \"unanswerable\":\n",
    "        assert item[\"relevant_sources\"] == []\n",
    "    else:\n",
    "        assert len(item[\"relevant_sources\"]) > 0\n",
    "        assert set(item[\"relevant_sources\"]).issubset(KNOWN_SOURCES)\n",
    "\n",
    "print(\"Dataset integrity checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f124a9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined (refactored).\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# E2) MODULAR EVALUATION FUNCTIONS (REFACTORED)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "# ── Helper: interpret Cohen's d magnitude ─────────────────────\n",
    "\n",
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Return a human-readable effect-size label for Cohen's d.\n",
    "\n",
    "    Thresholds follow Cohen (1988): |d| < 0.2 negligible,\n",
    "    0.2–0.5 small, 0.5–0.8 medium, > 0.8 large.\n",
    "    \"\"\"\n",
    "    ad = abs(d)\n",
    "    if ad < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif ad < 0.5:\n",
    "        return \"small\"\n",
    "    elif ad < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "\n",
    "# ── Helper: aggregate metrics from result rows ────────────────\n",
    "\n",
    "def aggregate_metrics(results, metric_key):\n",
    "    \"\"\"Extract a metric array from result rows and return summary stats.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dicts from run_generation_eval.\n",
    "        metric_key: Key name (e.g. 'rag_rouge_l', 'baseline_bert_score').\n",
    "\n",
    "    Returns:\n",
    "        Dict with mean, std, and the raw numpy array.\n",
    "    \"\"\"\n",
    "    arr = np.array([r[metric_key] for r in results], dtype=float)\n",
    "    return {\"mean\": float(np.mean(arr)), \"std\": float(np.std(arr, ddof=1)), \"values\": arr}\n",
    "\n",
    "\n",
    "def paired_test_and_effect(rag_arr, base_arr):\n",
    "    \"\"\"Run paired t-test and compute Cohen's d + 95 % CI.\n",
    "\n",
    "    Args:\n",
    "        rag_arr: 1-D numpy array of RAG scores.\n",
    "        base_arr: 1-D numpy array of baseline scores (same length).\n",
    "\n",
    "    Returns:\n",
    "        Dict with t_stat, p_value, cohens_d, ci_low, ci_high, mean_diff.\n",
    "    \"\"\"\n",
    "    diff = rag_arr - base_arr\n",
    "    n = len(diff)\n",
    "    diff_mean = float(np.mean(diff))\n",
    "    diff_std = float(np.std(diff, ddof=1))\n",
    "\n",
    "    if np.allclose(diff, 0.0):\n",
    "        t_stat, p_val = 0.0, 1.0\n",
    "    else:\n",
    "        t_stat, p_val = ttest_rel(rag_arr, base_arr)\n",
    "        t_stat, p_val = float(t_stat), float(p_val)\n",
    "\n",
    "    cohens_d = float(diff_mean / diff_std) if diff_std > 0 else 0.0\n",
    "    t_crit = float(t.ppf(0.975, df=n - 1))\n",
    "    sem = diff_std / np.sqrt(n)\n",
    "    ci_low = float(diff_mean - t_crit * sem)\n",
    "    ci_high = float(diff_mean + t_crit * sem)\n",
    "\n",
    "    return {\n",
    "        \"t_stat\": t_stat, \"p_value\": p_val, \"cohens_d\": cohens_d,\n",
    "        \"ci_low\": ci_low, \"ci_high\": ci_high, \"mean_diff\": diff_mean,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Generation evaluation loop ───────────────────────────────\n",
    "\n",
    "def run_generation_eval(dataset_subset, top_k=3, include_baseline=True, verbose=True):\n",
    "    \"\"\"Run RAG (and optionally baseline) generation on a dataset subset.\n",
    "\n",
    "    Returns a list of per-question result dicts.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(dataset_subset)\n",
    "\n",
    "    for i, item in enumerate(dataset_subset, 1):\n",
    "        q, ref = item[\"question\"], item[\"reference_answer\"]\n",
    "        rag_result = generate_rag_answer(q, top_k=top_k)\n",
    "        rag_scores = evaluate_answer(rag_result[\"answer\"], ref)\n",
    "\n",
    "        row = {\n",
    "            \"id\": item[\"id\"], \"split\": item[\"split\"], \"type\": item[\"type\"],\n",
    "            \"question\": q, \"reference\": ref,\n",
    "            \"relevant_sources\": item[\"relevant_sources\"],\n",
    "            \"rag_answer\": rag_result[\"answer\"],\n",
    "            \"rag_rouge_l\": float(rag_scores[\"rouge_l\"]),\n",
    "            \"rag_cosine\": float(rag_scores[\"cosine_similarity\"]),\n",
    "            \"rag_bert_score\": float(rag_scores[\"bert_score\"]),\n",
    "            \"sources\": rag_result.get(\"sources\", []),\n",
    "        }\n",
    "\n",
    "        if include_baseline:\n",
    "            bl = generate_baseline_answer(q)\n",
    "            bl_scores = evaluate_answer(bl[\"answer\"], ref)\n",
    "            row.update({\n",
    "                \"baseline_answer\": bl[\"answer\"],\n",
    "                \"baseline_rouge_l\": float(bl_scores[\"rouge_l\"]),\n",
    "                \"baseline_cosine\": float(bl_scores[\"cosine_similarity\"]),\n",
    "                \"baseline_bert_score\": float(bl_scores[\"bert_score\"]),\n",
    "            })\n",
    "        else:\n",
    "            row.update({\n",
    "                \"baseline_answer\": None,\n",
    "                \"baseline_rouge_l\": np.nan,\n",
    "                \"baseline_cosine\": np.nan,\n",
    "                \"baseline_bert_score\": np.nan,\n",
    "            })\n",
    "\n",
    "        results.append(row)\n",
    "        if verbose:\n",
    "            print(f\"[{i:02d}/{total:02d}] {item['id']} | type={item['type']} | top_k={top_k}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ── Top-k ablation (DEV only) ────────────────────────────────\n",
    "\n",
    "def run_topk_ablation(dev_subset, topk_values=(3, 5)):\n",
    "    \"\"\"Ablate top_k on DEV split and select best setting.\"\"\"\n",
    "    table, dev_results_by_topk = [], {}\n",
    "\n",
    "    print(\"=\" * 78)\n",
    "    print(\"Top-k Ablation on DEV (RAG only): top_k in\", set(topk_values))\n",
    "    print(\"=\" * 78)\n",
    "\n",
    "    for k in topk_values:\n",
    "        dev_results = run_generation_eval(dev_subset, top_k=k, include_baseline=False, verbose=False)\n",
    "        dev_results_by_topk[k] = dev_results\n",
    "        rouge_agg = aggregate_metrics(dev_results, \"rag_rouge_l\")\n",
    "        cos_agg = aggregate_metrics(dev_results, \"rag_cosine\")\n",
    "        table.append({\n",
    "            \"top_k\": k,\n",
    "            \"rag_rouge_mean\": rouge_agg[\"mean\"], \"rag_rouge_std\": rouge_agg[\"std\"],\n",
    "            \"rag_cosine_mean\": cos_agg[\"mean\"], \"rag_cosine_std\": cos_agg[\"std\"],\n",
    "        })\n",
    "\n",
    "    table = sorted(table, key=lambda x: x[\"top_k\"])\n",
    "    print(f\"{'top_k':<8}{'ROUGE-L mean':<16}{'ROUGE-L std':<14}{'Cosine mean':<14}{'Cosine std':<12}\")\n",
    "    print(\"-\" * 64)\n",
    "    for row in table:\n",
    "        print(f\"{row['top_k']:<8}{row['rag_rouge_mean']:<16.4f}{row['rag_rouge_std']:<14.4f}\"\n",
    "              f\"{row['rag_cosine_mean']:<14.4f}{row['rag_cosine_std']:<12.4f}\")\n",
    "\n",
    "    best = max(table, key=lambda x: (x[\"rag_rouge_mean\"], x[\"rag_cosine_mean\"], -x[\"rag_rouge_std\"]))\n",
    "    best_top_k = int(best[\"top_k\"])\n",
    "    print(f\"\\nSelected best_top_k on DEV: {best_top_k}\")\n",
    "\n",
    "    return {\"table\": table, \"best_top_k\": best_top_k, \"dev_results_by_topk\": dev_results_by_topk}\n",
    "\n",
    "\n",
    "# ── Statistical validation (answerable only + unanswerable analysis) ──\n",
    "\n",
    "def compute_statistical_validation(test_results):\n",
    "    \"\"\"Compute paired stats on ANSWERABLE test items only.\n",
    "\n",
    "    Prints a detailed summary with effect-size interpretation and a\n",
    "    statistical power disclaimer.  Unanswerable items are analyzed\n",
    "    separately.\n",
    "\n",
    "    Returns:\n",
    "        Dict with all statistical summaries.\n",
    "    \"\"\"\n",
    "    answerable = [r for r in test_results if r[\"type\"] != \"unanswerable\"]\n",
    "    unanswerable = [r for r in test_results if r[\"type\"] == \"unanswerable\"]\n",
    "    n_ans = len(answerable)\n",
    "\n",
    "    # ── Answerable: ROUGE-L ──\n",
    "    rag_rouge = aggregate_metrics(answerable, \"rag_rouge_l\")\n",
    "    base_rouge = aggregate_metrics(answerable, \"baseline_rouge_l\")\n",
    "    rouge_test = paired_test_and_effect(rag_rouge[\"values\"], base_rouge[\"values\"])\n",
    "\n",
    "    # ── Answerable: BERTScore ──\n",
    "    rag_bert = aggregate_metrics(answerable, \"rag_bert_score\")\n",
    "    base_bert = aggregate_metrics(answerable, \"baseline_bert_score\")\n",
    "    bert_test = paired_test_and_effect(rag_bert[\"values\"], base_bert[\"values\"])\n",
    "\n",
    "    # ── Unanswerable summary ──\n",
    "    unanswerable_summary = {}\n",
    "    if unanswerable:\n",
    "        u_rag = aggregate_metrics(unanswerable, \"rag_rouge_l\")\n",
    "        u_base = aggregate_metrics(unanswerable, \"baseline_rouge_l\")\n",
    "        unanswerable_summary = {\n",
    "            \"n\": len(unanswerable),\n",
    "            \"rag_rouge_mean\": u_rag[\"mean\"], \"baseline_rouge_mean\": u_base[\"mean\"],\n",
    "        }\n",
    "\n",
    "    out = {\n",
    "        \"n_test\": n_ans,\n",
    "        \"n_unanswerable\": len(unanswerable),\n",
    "        # ROUGE-L (answerable)\n",
    "        \"rag_mean\": rag_rouge[\"mean\"], \"rag_std\": rag_rouge[\"std\"],\n",
    "        \"baseline_mean\": base_rouge[\"mean\"], \"baseline_std\": base_rouge[\"std\"],\n",
    "        \"mean_diff\": rouge_test[\"mean_diff\"],\n",
    "        \"t_stat\": rouge_test[\"t_stat\"], \"p_value\": rouge_test[\"p_value\"],\n",
    "        \"cohens_d\": rouge_test[\"cohens_d\"],\n",
    "        \"ci_diff_low\": rouge_test[\"ci_low\"], \"ci_diff_high\": rouge_test[\"ci_high\"],\n",
    "        # BERTScore (answerable)\n",
    "        \"rag_bert_mean\": rag_bert[\"mean\"], \"rag_bert_std\": rag_bert[\"std\"],\n",
    "        \"baseline_bert_mean\": base_bert[\"mean\"], \"baseline_bert_std\": base_bert[\"std\"],\n",
    "        \"bert_mean_diff\": bert_test[\"mean_diff\"],\n",
    "        \"bert_t_stat\": bert_test[\"t_stat\"], \"bert_p_value\": bert_test[\"p_value\"],\n",
    "        \"bert_cohens_d\": bert_test[\"cohens_d\"],\n",
    "        # Unanswerable\n",
    "        \"unanswerable\": unanswerable_summary,\n",
    "    }\n",
    "\n",
    "    # ── Print answerable stats with interpretation ──\n",
    "    rouge_d_label = interpret_cohens_d(rouge_test[\"cohens_d\"])\n",
    "    bert_d_label = interpret_cohens_d(bert_test[\"cohens_d\"])\n",
    "    rouge_sig = \"statistically significant (p < 0.05)\" if rouge_test[\"p_value\"] < 0.05 else \"NOT statistically significant (p >= 0.05)\"\n",
    "    bert_sig = \"statistically significant (p < 0.05)\" if bert_test[\"p_value\"] < 0.05 else \"NOT statistically significant (p >= 0.05)\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 78)\n",
    "    print(f\"Statistical Validation — ANSWERABLE only (n = {n_ans})\")\n",
    "    print(\"=\" * 78)\n",
    "\n",
    "    print(f\"\\n[ROUGE-L]\")\n",
    "    print(f\"  RAG mean      = {rag_rouge['mean']:.4f}  (std = {rag_rouge['std']:.4f})\")\n",
    "    print(f\"  Baseline mean = {base_rouge['mean']:.4f}  (std = {base_rouge['std']:.4f})\")\n",
    "    print(f\"  Difference (Δ)= {rouge_test['mean_diff']:.4f}\")\n",
    "    print(f\"  95% CI        = [{rouge_test['ci_low']:.4f}, {rouge_test['ci_high']:.4f}]\")\n",
    "    print(f\"  t-statistic   = {rouge_test['t_stat']:.4f}\")\n",
    "    print(f\"  p-value       = {rouge_test['p_value']:.6f}  →  {rouge_sig}\")\n",
    "    print(f\"  Cohen's d     = {rouge_test['cohens_d']:.4f}  →  {rouge_d_label} effect\")\n",
    "\n",
    "    print(f\"\\n[BERTScore]\")\n",
    "    print(f\"  RAG mean      = {rag_bert['mean']:.4f}  (std = {rag_bert['std']:.4f})\")\n",
    "    print(f\"  Baseline mean = {base_bert['mean']:.4f}  (std = {base_bert['std']:.4f})\")\n",
    "    print(f\"  Difference (Δ)= {bert_test['mean_diff']:.4f}\")\n",
    "    print(f\"  t-statistic   = {bert_test['t_stat']:.4f}\")\n",
    "    print(f\"  p-value       = {bert_test['p_value']:.6f}  →  {bert_sig}\")\n",
    "    print(f\"  Cohen's d     = {bert_test['cohens_d']:.4f}  →  {bert_d_label} effect\")\n",
    "\n",
    "    print(f\"\\n{'─' * 78}\")\n",
    "    print(f\"⚠  STATISTICAL POWER DISCLAIMER\")\n",
    "    print(f\"   With n = {n_ans} answerable test items, this study has LOW statistical\")\n",
    "    print(f\"   power.  A non-significant p-value does NOT imply that RAG and\")\n",
    "    print(f\"   baseline perform equally; it may simply reflect insufficient sample\")\n",
    "    print(f\"   size.  Interpret these results as indicative, not definitive.\")\n",
    "    print(f\"{'─' * 78}\")\n",
    "\n",
    "    # ── Print unanswerable analysis ──\n",
    "    if unanswerable_summary:\n",
    "        print(f\"\\n{'=' * 78}\")\n",
    "        print(f\"Unanswerable Analysis (n = {unanswerable_summary['n']})\")\n",
    "        print(\"=\" * 78)\n",
    "        print(f\"  RAG ROUGE-L mean      = {unanswerable_summary['rag_rouge_mean']:.4f}\")\n",
    "        print(f\"  Baseline ROUGE-L mean = {unanswerable_summary['baseline_rouge_mean']:.4f}\")\n",
    "        print(\"  Interpretation: Both systems are expected to score low on unanswerable\")\n",
    "        print(\"  questions since the reference states the question cannot be answered.\")\n",
    "        print(\"  These items are excluded from the paired significance test.\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ── Retrieval-only evaluation ─────────────────────────────────\n",
    "\n",
    "def evaluate_retrieval_only(dataset_subset, k=5):\n",
    "    \"\"\"Compute Precision@k, Recall@k, and MRR on answerable items.\n",
    "\n",
    "    Args:\n",
    "        dataset_subset: List of evaluation items.\n",
    "        k: Number of chunks to retrieve per query.\n",
    "\n",
    "    Returns:\n",
    "        Dict with aggregate retrieval metrics and per-question breakdown.\n",
    "    \"\"\"\n",
    "    precision_scores, recall_scores, mrr_scores = [], [], []\n",
    "    per_question = []\n",
    "    n_skipped = 0\n",
    "\n",
    "    for item in dataset_subset:\n",
    "        relevant = item[\"relevant_sources\"]\n",
    "        if not relevant:\n",
    "            n_skipped += 1\n",
    "            continue\n",
    "\n",
    "        retrieved = retrieve(item[\"question\"], top_k=k)\n",
    "        retrieved_sources = [x[\"metadata\"][\"source\"] for x in retrieved]\n",
    "        relevant_set = set(relevant)\n",
    "        hits = set(retrieved_sources) & relevant_set\n",
    "\n",
    "        prec = len(hits) / float(k)\n",
    "        rec = len(hits) / float(len(relevant_set))\n",
    "        rr = next((1.0 / rank for rank, s in enumerate(retrieved_sources, 1) if s in relevant_set), 0.0)\n",
    "\n",
    "        precision_scores.append(prec)\n",
    "        recall_scores.append(rec)\n",
    "        mrr_scores.append(rr)\n",
    "        per_question.append({\n",
    "            \"id\": item[\"id\"], \"question\": item[\"question\"],\n",
    "            \"retrieved_sources\": retrieved_sources, \"relevant_sources\": relevant,\n",
    "            \"precision_at_k\": prec, \"recall_at_k\": rec, \"reciprocal_rank\": rr,\n",
    "        })\n",
    "\n",
    "    if not per_question:\n",
    "        raise ValueError(\"No answerable questions for retrieval evaluation.\")\n",
    "\n",
    "    summary = {\n",
    "        \"k\": k, \"n_evaluated\": len(per_question), \"n_skipped_unanswerable\": n_skipped,\n",
    "        \"precision_at_k\": float(np.mean(precision_scores)),\n",
    "        \"recall_at_k\": float(np.mean(recall_scores)),\n",
    "        \"mrr\": float(np.mean(mrr_scores)),\n",
    "        \"per_question\": per_question,\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 78)\n",
    "    print(f\"Retrieval-only Evaluation @k={k}\")\n",
    "    print(\"=\" * 78)\n",
    "    print(f\"Evaluated={summary['n_evaluated']} | Skipped unanswerable={n_skipped}\")\n",
    "    print(f\"Precision@{k}: {summary['precision_at_k']:.4f}\")\n",
    "    print(f\"Recall@{k}:    {summary['recall_at_k']:.4f}\")\n",
    "    print(f\"MRR:           {summary['mrr']:.4f}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ── Failure analysis ──────────────────────────────────────────\n",
    "\n",
    "def print_failure_analysis(results, n=3):\n",
    "    \"\"\"Print and return the n lowest-scoring RAG ROUGE-L cases.\n",
    "\n",
    "    Args:\n",
    "        results: List of per-question result dicts.\n",
    "        n: Number of worst cases to display.\n",
    "\n",
    "    Returns:\n",
    "        List of n failure-case dicts sorted by ascending ROUGE-L.\n",
    "    \"\"\"\n",
    "    assert 0 < n <= len(results), \"n must be in [1, len(results)].\"\n",
    "\n",
    "    lowest = sorted(results, key=lambda x: x[\"rag_rouge_l\"])[:n]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 78)\n",
    "    print(f\"Failure Analysis: Lowest {n} RAG ROUGE-L Cases\")\n",
    "    print(\"=\" * 78)\n",
    "\n",
    "    records = []\n",
    "    for rank, item in enumerate(lowest, 1):\n",
    "        print(f\"\\nCase {rank} | {item['id']} | RAG={item['rag_rouge_l']:.4f} | Base={item['baseline_rouge_l']:.4f}\")\n",
    "        print(f\"  Q: {item['question']}\")\n",
    "        print(f\"  RAG: {item['rag_answer']}\")\n",
    "        print(f\"  Base: {item['baseline_answer']}\")\n",
    "        print(f\"  Ref: {item['reference']}\")\n",
    "        records.append({\n",
    "            \"id\": item[\"id\"], \"question\": item[\"question\"],\n",
    "            \"rag_rouge_l\": float(item[\"rag_rouge_l\"]),\n",
    "            \"baseline_rouge_l\": float(item[\"baseline_rouge_l\"]),\n",
    "            \"rag_answer\": item[\"rag_answer\"],\n",
    "            \"baseline_answer\": item[\"baseline_answer\"],\n",
    "            \"reference_answer\": item[\"reference\"],\n",
    "        })\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "# ── Experimental results text builder ─────────────────────────\n",
    "\n",
    "def build_experimental_results_section(stats_out, ablation_out, retrieval_out):\n",
    "    \"\"\"Build an academic-style experimental results section with interpretation.\n",
    "\n",
    "    Includes Cohen's d interpretation, p-value explanation, and a\n",
    "    statistical power disclaimer.\n",
    "    \"\"\"\n",
    "    rouge_sig = \"statistically significant\" if stats_out[\"p_value\"] < 0.05 else \"not statistically significant\"\n",
    "    bert_sig = \"statistically significant\" if stats_out[\"bert_p_value\"] < 0.05 else \"not statistically significant\"\n",
    "    rouge_d_label = interpret_cohens_d(stats_out[\"cohens_d\"])\n",
    "    bert_d_label = interpret_cohens_d(stats_out[\"bert_cohens_d\"])\n",
    "    k = ablation_out[\"best_top_k\"]\n",
    "\n",
    "    text = (\n",
    "        f\"We evaluated a fixed RAG pipeline against a parametric baseline using identical \"\n",
    "        f\"prompt templates and decoding parameters (max_new_tokens=220, min_new_tokens=60, \"\n",
    "        f\"num_beams=4, length_penalty=1.0, no_repeat_ngram_size=3). The only experimental \"\n",
    "        f\"manipulation was the presence or absence of retrieved context.\\n\\n\"\n",
    "        f\"The top-k retrieval parameter was tuned on the dev split (k ∈ {{3, 5}}); the \"\n",
    "        f\"selected value (k={k}) was frozen before test-set evaluation. Inferential \"\n",
    "        f\"analysis was conducted on n={stats_out['n_test']} answerable held-out test \"\n",
    "        f\"questions; {stats_out.get('n_unanswerable', 0)} unanswerable items were \"\n",
    "        f\"excluded from significance testing.\\n\\n\"\n",
    "        f\"ROUGE-L: RAG mean={stats_out['rag_mean']:.4f} (std={stats_out['rag_std']:.4f}) \"\n",
    "        f\"vs. baseline mean={stats_out['baseline_mean']:.4f} \"\n",
    "        f\"(std={stats_out['baseline_std']:.4f}), Δ={stats_out['mean_diff']:.4f}. \"\n",
    "        f\"Paired t-test: t={stats_out['t_stat']:.4f}, p={stats_out['p_value']:.6f} \"\n",
    "        f\"— {rouge_sig}. \"\n",
    "        f\"Cohen's d={stats_out['cohens_d']:.4f} ({rouge_d_label} effect), 95% CI \"\n",
    "        f\"[{stats_out['ci_diff_low']:.4f}, {stats_out['ci_diff_high']:.4f}].\\n\\n\"\n",
    "        f\"BERTScore: RAG mean={stats_out['rag_bert_mean']:.4f} \"\n",
    "        f\"(std={stats_out['rag_bert_std']:.4f}) vs. baseline \"\n",
    "        f\"mean={stats_out['baseline_bert_mean']:.4f} \"\n",
    "        f\"(std={stats_out['baseline_bert_std']:.4f}), Δ={stats_out['bert_mean_diff']:.4f}. \"\n",
    "        f\"Paired t-test: t={stats_out['bert_t_stat']:.4f}, \"\n",
    "        f\"p={stats_out['bert_p_value']:.6f} — {bert_sig}. \"\n",
    "        f\"Cohen's d={stats_out['bert_cohens_d']:.4f} ({bert_d_label} effect).\\n\\n\"\n",
    "        f\"Retrieval evaluation at k={retrieval_out['k']}: \"\n",
    "        f\"Precision@k={retrieval_out['precision_at_k']:.4f}, \"\n",
    "        f\"Recall@k={retrieval_out['recall_at_k']:.4f}, \"\n",
    "        f\"MRR={retrieval_out['mrr']:.4f}.\\n\\n\"\n",
    "        f\"IMPORTANT CAVEAT: With only n={stats_out['n_test']} answerable test items, \"\n",
    "        f\"the statistical power of these tests is low. Non-significant p-values should \"\n",
    "        f\"not be interpreted as evidence of equivalence between RAG and baseline; they \"\n",
    "        f\"may reflect insufficient sample size. These results are indicative of \"\n",
    "        f\"directional trends and should be replicated on a larger evaluation set before \"\n",
    "        f\"drawing definitive conclusions.\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined (refactored).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "332fbbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "Top-k Ablation on DEV (RAG only): top_k in {3, 5}\n",
      "==============================================================================\n",
      "top_k   ROUGE-L mean    ROUGE-L std   Cosine mean   Cosine std  \n",
      "----------------------------------------------------------------\n",
      "3       0.0926          0.0586        0.4011        0.0911      \n",
      "5       0.0926          0.0586        0.4011        0.0911      \n",
      "\n",
      "Selected best_top_k on DEV: 3\n",
      "\n",
      "Frozen best_top_k (chosen on DEV only): 3\n",
      "[01/10] S15 | type=standard | top_k=3\n",
      "[02/10] S16 | type=standard | top_k=3\n",
      "[03/10] S17 | type=standard | top_k=3\n",
      "[04/10] S18 | type=standard | top_k=3\n",
      "[05/10] S19 | type=standard | top_k=3\n",
      "[06/10] S20 | type=standard | top_k=3\n",
      "[07/10] M04 | type=multi_hop | top_k=3\n",
      "[08/10] M05 | type=multi_hop | top_k=3\n",
      "[09/10] U04 | type=unanswerable | top_k=3\n",
      "[10/10] U05 | type=unanswerable | top_k=3\n",
      "\n",
      "==============================================================================\n",
      "Statistical Validation — ANSWERABLE only (n = 8)\n",
      "==============================================================================\n",
      "\n",
      "[ROUGE-L]\n",
      "  RAG mean      = 0.0355  (std = 0.0458)\n",
      "  Baseline mean = 0.0409  (std = 0.0406)\n",
      "  Difference (Δ)= -0.0054\n",
      "  95% CI        = [-0.0516, 0.0408]\n",
      "  t-statistic   = -0.2751\n",
      "  p-value       = 0.791212  →  NOT statistically significant (p >= 0.05)\n",
      "  Cohen's d     = -0.0972  →  negligible effect\n",
      "\n",
      "[BERTScore]\n",
      "  RAG mean      = 0.0109  (std = 0.0685)\n",
      "  Baseline mean = -0.1110  (std = 0.1290)\n",
      "  Difference (Δ)= 0.1218\n",
      "  t-statistic   = 2.5921\n",
      "  p-value       = 0.035834  →  statistically significant (p < 0.05)\n",
      "  Cohen's d     = 0.9165  →  large effect\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────\n",
      "⚠  STATISTICAL POWER DISCLAIMER\n",
      "   With n = 8 answerable test items, this study has LOW statistical\n",
      "   power.  A non-significant p-value does NOT imply that RAG and\n",
      "   baseline perform equally; it may simply reflect insufficient sample\n",
      "   size.  Interpret these results as indicative, not definitive.\n",
      "──────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "==============================================================================\n",
      "Unanswerable Analysis (n = 2)\n",
      "==============================================================================\n",
      "  RAG ROUGE-L mean      = 0.1587\n",
      "  Baseline ROUGE-L mean = 0.0362\n",
      "  Interpretation: Both systems are expected to score low on unanswerable\n",
      "  questions since the reference states the question cannot be answered.\n",
      "  These items are excluded from the paired significance test.\n",
      "\n",
      "==============================================================================\n",
      "Retrieval-only Evaluation @k=3\n",
      "==============================================================================\n",
      "Evaluated=8 | Skipped unanswerable=2\n",
      "Precision@3: 0.1667\n",
      "Recall@3:    0.5000\n",
      "MRR:           0.3333\n",
      "\n",
      "==============================================================================\n",
      "Failure Analysis: Lowest 3 RAG ROUGE-L Cases\n",
      "==============================================================================\n",
      "\n",
      "Case 1 | S16 | RAG=0.0000 | Base=0.0741\n",
      "  Q: Why use Adam optimizer?\n",
      "  RAG: in DNNs you are sampling from a mini-batch, there is going to be a lot of randomness in the forward pass. window just to track the previous n gradients because it would take too much memory. Comparing RMSProp vs AdaGrad usually ends up stopping too early\n",
      "  Base: Can be determined from the information provided. br />bl /bl/n/n /b/f a href name=\"Adam Optimizer\"/a>  br /\n",
      "  Ref: Adam adapts learning rates per parameter.\n",
      "\n",
      "Case 2 | S19 | RAG=0.0000 | Base=0.0377\n",
      "  Q: What is stride in CNN?\n",
      "  RAG: The answer cannot be determined from the provided material. It's not possible to answer this question because there is not enough information to answer the above question. It may be based on your own research or information contained in a textbook, but we strongly suggest that you contact a professional before attempting to answer these types of questions.\n",
      "  Base: The answer cannot be determined from the information provided. \"Cnn\" stands for \"Channel News and Current Affairs\" in the United States. CNN stands for CNN International. CNN is headquartered in New York City. CNN's headquarters are located in Washington, D.C., United States of America.\n",
      "  Ref: Stride controls filter movement and spatial downsampling.\n",
      "\n",
      "Case 3 | S20 | RAG=0.0000 | Base=0.0000\n",
      "  Q: Why 1x1 convolution?\n",
      "  RAG: Usually signals are 2-dimensional so 11 convolutions do not make sense (its just pointwise scaling). However, in ConvNets this is not the case because one must remember that we operate over 3-dimensional volumes, and that the lters always extend through the full depth of the input volume\n",
      "  Base: the answer cannot be determined from the provided material. br />bl / nb/br/nn/b / 1x1 convolution/a> br /nb  1x 1 convolution\n",
      "  Ref: It mixes channel information efficiently.\n",
      "\n",
      "All evaluation assertions passed.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# E3) STRICT EXECUTION PROTOCOL (DEV ABLATION → FREEZE → TEST)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "# 1) Tune top_k on DEV only (no test access)\n",
    "ablation_out = run_topk_ablation(dev_set, topk_values=(3, 5))\n",
    "\n",
    "# 2) Freeze best_top_k\n",
    "best_top_k = ablation_out[\"best_top_k\"]\n",
    "print(f\"\\nFrozen best_top_k (chosen on DEV only): {best_top_k}\")\n",
    "\n",
    "# 3) Final generation evaluation on TEST only\n",
    "test_results = run_generation_eval(test_set, top_k=best_top_k, include_baseline=True, verbose=True)\n",
    "\n",
    "# Assertions: metric ranges + no NaNs/infs\n",
    "rouge_matrix = np.array(\n",
    "    [[r[\"rag_rouge_l\"], r[\"baseline_rouge_l\"]] for r in test_results], dtype=float\n",
    ")\n",
    "cosine_matrix = np.array(\n",
    "    [[r[\"rag_cosine\"], r[\"baseline_cosine\"]] for r in test_results], dtype=float\n",
    ")\n",
    "bert_matrix = np.array(\n",
    "    [[r[\"rag_bert_score\"], r[\"baseline_bert_score\"]] for r in test_results], dtype=float\n",
    ")\n",
    "assert np.all(np.isfinite(rouge_matrix)) and np.all(np.isfinite(cosine_matrix)) and np.all(np.isfinite(bert_matrix)), \\\n",
    "    \"NaN/inf detected in test metrics.\"\n",
    "assert np.all((rouge_matrix >= 0.0) & (rouge_matrix <= 1.0)), \"ROUGE-L outside [0,1].\"\n",
    "assert np.all((cosine_matrix >= -1.0) & (cosine_matrix <= 1.0)), \"Cosine outside [-1,1].\"\n",
    "assert np.all((bert_matrix >= -1.0) & (bert_matrix <= 1.0)), \"BERTScore outside [-1,1].\"\n",
    "\n",
    "# 4) Statistical validation (answerable only; unanswerable analyzed separately)\n",
    "stats_out = compute_statistical_validation(test_results)\n",
    "\n",
    "# Assertions: finite stats + valid CI\n",
    "assert stats_out[\"ci_diff_low\"] <= stats_out[\"ci_diff_high\"], \"Invalid CI.\"\n",
    "for key in [\"rag_mean\", \"rag_std\", \"baseline_mean\", \"baseline_std\", \"mean_diff\",\n",
    "            \"t_stat\", \"p_value\", \"cohens_d\",\n",
    "            \"rag_bert_mean\", \"rag_bert_std\", \"baseline_bert_mean\", \"baseline_bert_std\",\n",
    "            \"bert_mean_diff\", \"bert_t_stat\", \"bert_p_value\", \"bert_cohens_d\"]:\n",
    "    assert np.isfinite(stats_out[key]), f\"Non-finite: {key}\"\n",
    "\n",
    "# 5) Retrieval-only evaluation on TEST (answerable items)\n",
    "retrieval_out = evaluate_retrieval_only(test_set, k=best_top_k)\n",
    "\n",
    "for key in [\"precision_at_k\", \"recall_at_k\", \"mrr\"]:\n",
    "    assert np.isfinite(retrieval_out[key]) and 0.0 <= retrieval_out[key] <= 1.0, \\\n",
    "        f\"Retrieval metric {key} invalid.\"\n",
    "\n",
    "# 6) Failure analysis\n",
    "failure_cases = print_failure_analysis(test_results, n=3)\n",
    "assert len(failure_cases) == 3\n",
    "assert [x[\"rag_rouge_l\"] for x in failure_cases] == sorted(x[\"rag_rouge_l\"] for x in failure_cases)\n",
    "\n",
    "print(\"\\nAll evaluation assertions passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4733d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================================\n",
      "Experimental Results (Report-Ready)\n",
      "==============================================================================\n",
      "We evaluated a fixed RAG pipeline against a parametric baseline using identical prompt templates and decoding parameters (max_new_tokens=220, min_new_tokens=60, num_beams=4, length_penalty=1.0, no_repeat_ngram_size=3). The only experimental manipulation was the presence or absence of retrieved context.\n",
      "\n",
      "The top-k retrieval parameter was tuned on the dev split (k ∈ {3, 5}); the selected value (k=3) was frozen before test-set evaluation. Inferential analysis was conducted on n=8 answerable held-out test questions; 2 unanswerable items were excluded from significance testing.\n",
      "\n",
      "ROUGE-L: RAG mean=0.0355 (std=0.0458) vs. baseline mean=0.0409 (std=0.0406), Δ=-0.0054. Paired t-test: t=-0.2751, p=0.791212 — not statistically significant. Cohen's d=-0.0972 (negligible effect), 95% CI [-0.0516, 0.0408].\n",
      "\n",
      "BERTScore: RAG mean=0.0109 (std=0.0685) vs. baseline mean=-0.1110 (std=0.1290), Δ=0.1218. Paired t-test: t=2.5921, p=0.035834 — statistically significant. Cohen's d=0.9165 (large effect).\n",
      "\n",
      "Retrieval evaluation at k=3: Precision@k=0.1667, Recall@k=0.5000, MRR=0.3333.\n",
      "\n",
      "IMPORTANT CAVEAT: With only n=8 answerable test items, the statistical power of these tests is low. Non-significant p-values should not be interpreted as evidence of equivalence between RAG and baseline; they may reflect insufficient sample size. These results are indicative of directional trends and should be replicated on a larger evaluation set before drawing definitive conclusions.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# E4) EXPERIMENTAL RESULTS SECTION (ACADEMIC STYLE, 2-3 PARAGRAPHS)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "experimental_results_text = build_experimental_results_section(stats_out, ablation_out, retrieval_out)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 78)\n",
    "print(\"Experimental Results (Report-Ready)\")\n",
    "print(\"=\" * 78)\n",
    "print(experimental_results_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
